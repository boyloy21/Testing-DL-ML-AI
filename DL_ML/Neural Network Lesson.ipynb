{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Activative function  \n",
    "1. Binary sigmoidal function: This activation function performs input editing between 0 and 1.\\\n",
    "    $\\sigma(x) = sigm(x)= \\frac{1}{1+e^{-x}}$\n",
    "2. Bipolar sigmoidal function − This activation function performs input editing between -1 and 1. It can be positive or negative in nature.\\\n",
    "    $\\sigma(x) = sigm(x) = \\frac{1 - e^x}{1 + e^x}$\n",
    "    \n",
    "3. Threshold Function : The threshold function is used when you don’t want to worry about the uncertainty in the middle.\\\n",
    "    $\\sigma(x) = \\begin{bmatrix}\n",
    "        1 & ,if x>=0 \\\\\n",
    "        0 & ,if x<0\n",
    "    \\end{bmatrix}$\n",
    "4. ReLU (rectified linear unit) Function : The ReLU (rectified linear unit) function gives the value but says if it’s over 1, then it will just be 1, and if it’s less than 0, it will just be 0. The ReLU function is most commonly used these days. \\\n",
    "    $\\sigma(x) = max(0,x)$\n",
    "5. Hyperbolic Tangent Function : The hyperbolic tangent function is similar to the sigmoid function but has a range of -1 to 1. \\\n",
    "    $\\sigma(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$\n",
    "6. Softmax is an activation function that is commonly used in the output layer of neural networks, particularly for multi-class classification problems.\n",
    "    $\\sigma(z) =\\frac{e^{z_i}}{\\sum_{i=1}^N e^{z_i}}$ where z is the vector of raw outputs from the neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Rule :\n",
    "  _ Discription : The chain rule of calculus is a foundational concept that allows us to find the derivative of a composite function.\\\n",
    "  _ Formular: $\\frac{d}{dx}(f(g(x))=\\frac{df(g)}{dg}\\frac{dg}{dx}$\\\n",
    "  _ Residual = Observed - Predicted where: Predicted = Intercept + (1 x weight)\n",
    "  $=> \\frac{dResidual}{dIntercept} = 0 + (-1)$\\\n",
    "  $=> y = Residual^2 = (Obseved - Predicted)^2$\\\n",
    "  $=> \\frac{dResidual^2}{dIntercept} = -2*(Obseved - Intercept + (1 * weight))$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent:\n",
    "Gradient descent is an optimization algorithm used in machine learning to minimize the cost function by iteratively adjusting parameters in the direction of the negative gradient, aiming to find the optimal set of parameters.\n",
    "+ Step1: Start by initializing the parameters(weights and biases) of the model with random values or predefined values.\n",
    "+ Step2: The gradient is a vector that points in the direction of the steepest increase of the function.\n",
    "+ Step3: Update the parameter in the opposite direction of the gradient to minimize the loos function.The update rule for each parameter $\\theta$ is : $\\theta = \\theta - \\alpha\\Delta_{\\theta}J(\\theta)$ \n",
    "Where: .$\\alpha$ is the learning rate, which determines the step size of each update,\\\n",
    ". $J(\\theta)$ is the cost function\n",
    ". $\\Delta_{\\theta}J(\\theta)$ is the gradient of the cost funtion with respect to the parameters.\\\n",
    "+ Step4: Type fo Gradient Descent you want to choose:\\\n",
    "  _ Batch gradient Descent: Batch Gradient Descent involves calculations over the full training set at each step as a result of which it is very slow on very large training data.\n",
    "    . $\\theta_j = \\theta_j - \\alpha\\frac{\\partial}{\\partial{\\theta_j}}J(\\theta)$, \\\n",
    "    .$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y_i} - y_i)X_j^i$\\\n",
    "  _ Stochastic Gradient Descent: \n",
    "    . Sum of squared residuals = (observedHeight - (intercept+slope*Weight)^2\n",
    "    .$\\frac{d}{d intercept}Sum fo squared residual= -2(Height - (intercept + slope * weight))$\n",
    "     .$\\frac{d}{d slope}Sum of squared residuals = -2 * Weight(Height - (intercept + slope*Weight))$\n",
    "     .$Step Size_{intercept} = \\frac{d}{d intercept}Sum fo squared residual * Learning Rate$\n",
    "     .$Step Size_{Slope} = \\frac{d}{d slope}Sum fo squared residual * Learning Rate$\n",
    "      Where: Weight is input , Height is output.\n",
    "      .New intercept = \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Propagtion:  \n",
    "_ Feedforward Neural Network is the simplest neural network. It is called Feedforward because information flows forward from Inputs -> hidden layers -> outputs. There are no feedback connections. The model feeds every output to the next layers and keeps moving forward.\\\n",
    "_ Forumular: $\\sigma(z) = \\sigma(b + \\sum_{i=1}^N w_ix_i )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation:\n",
    "+ Step1: We first select an error or cost function that calculates the error between predicted and actual: $\\mathbf{Cost = (Y_{pred} - Y_{act})^2}$\n",
    "+ Step2: Partial Derivative : $\\frac{\\partial{Cost}}{\\partial{w_1}}= \\frac{\\partial{Cost}}{\\partial{Y_{pred}}}*\\frac{\\partial{Y_{pred}}}{\\partial{g_1}}*\\frac{\\partial{g_1}}{\\partial{z_1}}*\\frac{\\partial{z_1}}{\\partial{w_1}}$\\\n",
    "_ $\\frac{\\partial{Cost}}{\\partial{Y_{pred}}} = 2(Y_{pred} - Y_{act})$\\\n",
    "_ $\\frac{\\partial{Y_pred}}{\\partial{Y_{g_1}}} = w_7 , \\hspace{0.3cm} Y_{pred} = w_7*g_1 + w_8*g_2 + b_3 $ \\\n",
    "_ $\\frac{\\partial{g_1}}{\\partial{z_1}} = g_1*(1-g_1)=(\\frac{1}{1+e^{-z_1}})*(1-\\frac{1}{1+e^{-z_1}}) $\n",
    "+ Step3: Update Weights : $w_n^+ = w_n - \\eta \\frac{\\partial{cost}}{\\partial{w_n}}$ \\\n",
    "_ $w_1^+ = w1 - \\eta \\frac{\\partial{cost}}{\\partial{w_1}}$\\\n",
    "_ $w_2^+ = w2 - \\eta \\frac{\\partial{cost}}{\\partial{w_2}}$\\\n",
    "_ $b_1^+ = b_1 - \\eta \\frac{\\partial{cost}}{\\partial{b_1}}$\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def bipolar_sigmoid(x):\n",
    "    return (1-np.exp(-x))/(1+np.exp(-x))\n",
    "\n",
    "def threshold(x):\n",
    "    if x >= 0:\n",
    "        phi = 1\n",
    "    elif x < 0:\n",
    "        phi = 0\n",
    "    return phi\n",
    "def Relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def hyperbolic_tangent(x):\n",
    "    return (1-np.exp(-2*x))/(1+np.exp(-2*x))\n",
    "\n",
    "def softmax(y):\n",
    "    ex = np.exp(y)\n",
    "    return ex/(sum(ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{pred} = sigmoid(w2*sigmoid(w1*x+b1)+b2)$\\\n",
    "$\\frac{\\partial{j}}{\\partial{w}}= \\frac{\\partial{y_{pred}}}{\\partial{w}} - \\frac{\\partial{y_{actual}}}{\\partial{w}}$\\\n",
    "Where: $y_{actual}$ is a constant value, $\\frac{\\partial{y_{actual}}}{\\partial{w}}=0$\\\n",
    "$\\frac{\\partial{j(w)}}{\\partial{w}}=\\frac{\\partial{y_{pred}}}{\\partial{w}}$\\\n",
    "$\\frac{\\partial{y_{pred}}}{\\partial{w1}}=\\frac{\\partial{outer}}{\\partial{inner}}*\\frac{\\partial{inner}}{\\partial{w1}}$\\\n",
    "Whre: outer = sigmoid(x) and inner = constant2 * sigmoid(constant1 * x)\\\n",
    "$\\frac{\\partial{outer}}{\\partial{inner}} = \\frac{\\partial{sigmoid(y)}}{\\partial{y}}=sigmoid(y)*(1-sigmoid(y))$\\\n",
    "$\\frac{\\partial{inner}}{\\partial{w1}}=\\frac{\\partial{(w2*sigmoid(w1*x))}}{\\partial{w1}}=w2*x*sigmoid(w1*x)*(1-sigmoid(w1*x))$\\\n",
    "$\\frac{\\partial{inner}}{\\partial{w2}}=\\frac{\\partial{(w2*sigmoid(w1*x))}}{\\partial{w2}}=sigmoid(w1*x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Rule\n",
    "def chain_rule(x,y,w1,w2):\n",
    "    for i in range(len(x)):\n",
    "        y_predic = sigmoid(w2*sigmoid(w1*x[i]))\n",
    "        douter_inner = sigmoid(y[i])*(1-sigmoid(y[i]))\n",
    "        dinner_w1 = w2*x[i]*sigmoid(w1*x[i])*(1-sigmoid(w1*x[i]))\n",
    "        dinner_w2 = sigmoid(w1*x[i])\n",
    "        dypred_w1 = douter_inner * dinner_w1\n",
    "        dypred_w2 = douter_inner * dinner_w2\n",
    "    return dypred_w1, dypred_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent \n",
    "def gradient_decent(x, y):\n",
    "    # Initial weight and bias\n",
    "    w1 = 0\n",
    "    w2 = 0\n",
    "    b_curr = 0\n",
    "    # Set steppest to increase or learning rate\n",
    "    iteration = 100\n",
    "    n = len(x)\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    for i in range(iteration):\n",
    "        dypred_w1, dypred_w2 = chain_rule(x,y,w1,w2)\n",
    "        w1 = w1 - learning_rate*dypred_w1\n",
    "        w2 = w2 - learning_rate*dypred_w2\n",
    "#         cost = (1/n)* sum([val**2 for val in (y-y_predicted)])\n",
    "#         md = -(2/n)*sum(x*(y-y_predicted))\n",
    "#         bd = -(2/n)*sum(y - y_predicted)\n",
    "#         m_curr = m_curr - learning_rate * md\n",
    "#         b_curr = b_curr - learning_rate * bd\n",
    "        print (\"w1 {}, w2 {}, cost {} iteration {}\".format(w1,w2,dypred_w1, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [1,2,3,4,5,6,7]\n",
    "# y = [2.2,4.4,6.3,8.2,10.5,11.8,14.3]\n",
    "# gradient_decent(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back Propagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
